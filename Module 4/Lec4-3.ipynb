{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0540,  0.0027,  0.0965,  ..., -0.0208, -0.0392,  0.0374],\n",
       "        [ 0.0329, -0.0074,  0.0568,  ..., -0.0303, -0.0554,  0.0392],\n",
       "        [ 0.0187, -0.0077,  0.0778,  ...,  0.0313,  0.0067,  0.0371],\n",
       "        ...,\n",
       "        [-0.0097, -0.0534,  0.0567,  ..., -0.0285, -0.0168, -0.0027],\n",
       "        [ 0.0098, -0.0306,  0.0328,  ..., -0.0407, -0.0280, -0.0087],\n",
       "        [ 0.0314, -0.0398,  0.0219,  ..., -0.0092,  0.0210,  0.0214]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, layer_size=[512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128*128*3  # number of channel\n",
    "        for s in layer_size:\n",
    "            layers.append(torch.nn.Linear(c, s))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102))\n",
    "\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "net = MyModel()\n",
    "x = torch.randn(10, 3, 128, 128)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0501, -0.0281,  0.0328,  ...,  0.0765,  0.0261,  0.0109],\n",
       "        [ 0.0390, -0.0766,  0.0622,  ...,  0.0773, -0.0082,  0.0123],\n",
       "        [ 0.0679, -0.0566, -0.0129,  ...,  0.0461,  0.0135, -0.0020],\n",
       "        ...,\n",
       "        [ 0.0884, -0.0320,  0.0412,  ...,  0.0822, -0.0115,  0.0615],\n",
       "        [ 0.0412, -0.0194,  0.0123,  ...,  0.0929, -0.0107,  0.0389],\n",
       "        [ 0.0588, -0.0306,  0.0452,  ...,  0.0779, -0.0029,  0.0372]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModelNoBias(torch.nn.Module):\n",
    "    def __init__(self, layer_size=[512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128*128*3  # number of channel\n",
    "        for s in layer_size:\n",
    "            layers.append(torch.nn.Linear(c, s, bias=False))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102, bias=False))\n",
    "\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "net = MyModel()\n",
    "x = torch.randn(10, 3, 128, 128)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net0(x).norm()=tensor(18.3989, grad_fn=<LinalgVectorNormBackward0>)\n",
      "net1(x).norm()=tensor(7.2410, grad_fn=<LinalgVectorNormBackward0>)\n",
      "net2(x).norm()=tensor(3.1070, grad_fn=<LinalgVectorNormBackward0>)\n",
      "0 netn(x).norm()=tensor(18.6123, grad_fn=<LinalgVectorNormBackward0>)\n",
      "1 netn(x).norm()=tensor(7.2068, grad_fn=<LinalgVectorNormBackward0>)\n",
      "2 netn(x).norm()=tensor(3.4652, grad_fn=<LinalgVectorNormBackward0>)\n",
      "3 netn(x).norm()=tensor(1.5229, grad_fn=<LinalgVectorNormBackward0>)\n",
      "4 netn(x).norm()=tensor(0.9647, grad_fn=<LinalgVectorNormBackward0>)\n",
      "5 netn(x).norm()=tensor(0.8190, grad_fn=<LinalgVectorNormBackward0>)\n",
      "6 netn(x).norm()=tensor(0.9076, grad_fn=<LinalgVectorNormBackward0>)\n",
      "7 netn(x).norm()=tensor(0.8148, grad_fn=<LinalgVectorNormBackward0>)\n",
      "8 netn(x).norm()=tensor(0.9776, grad_fn=<LinalgVectorNormBackward0>)\n",
      "9 netn(x).norm()=tensor(0.8660, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net0 = MyModel([])\n",
    "x = torch.randn(10, 3, 128, 128)\n",
    "print(f\"{net0(x).norm()=}\")\n",
    "net1 = MyModel([512])\n",
    "print(f\"{net1(x).norm()=}\")\n",
    "net2 = MyModel([512, 512])\n",
    "print(f\"{net2(x).norm()=}\")\n",
    "\n",
    "for n in range(10):\n",
    "    netn = MyModel([512] * n)\n",
    "    print(f\"{n} {netn(x).norm()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 netn(x).norm()=tensor(18.0155, grad_fn=<LinalgVectorNormBackward0>)\n",
      "1 netn(x).norm()=tensor(7.1321, grad_fn=<LinalgVectorNormBackward0>)\n",
      "2 netn(x).norm()=tensor(3.0214, grad_fn=<LinalgVectorNormBackward0>)\n",
      "3 netn(x).norm()=tensor(1.2086, grad_fn=<LinalgVectorNormBackward0>)\n",
      "4 netn(x).norm()=tensor(0.5479, grad_fn=<LinalgVectorNormBackward0>)\n",
      "5 netn(x).norm()=tensor(0.2147, grad_fn=<LinalgVectorNormBackward0>)\n",
      "6 netn(x).norm()=tensor(0.0866, grad_fn=<LinalgVectorNormBackward0>)\n",
      "7 netn(x).norm()=tensor(0.0344, grad_fn=<LinalgVectorNormBackward0>)\n",
      "8 netn(x).norm()=tensor(0.0118, grad_fn=<LinalgVectorNormBackward0>)\n",
      "9 netn(x).norm()=tensor(0.0062, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 128, 128)\n",
    "\n",
    "for n in range(10):\n",
    "    netn = MyModelNoBias([512] * n)\n",
    "    print(f\"{n} {netn(x).norm()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 netn(x).norm()=tensor(18.5365, grad_fn=<LinalgVectorNormBackward0>)\n",
      "1 netn(x).norm()=tensor(12.9939, grad_fn=<LinalgVectorNormBackward0>)\n",
      "2 netn(x).norm()=tensor(13.2019, grad_fn=<LinalgVectorNormBackward0>)\n",
      "3 netn(x).norm()=tensor(12.9503, grad_fn=<LinalgVectorNormBackward0>)\n",
      "4 netn(x).norm()=tensor(13.4037, grad_fn=<LinalgVectorNormBackward0>)\n",
      "5 netn(x).norm()=tensor(12.8535, grad_fn=<LinalgVectorNormBackward0>)\n",
      "6 netn(x).norm()=tensor(12.9624, grad_fn=<LinalgVectorNormBackward0>)\n",
      "7 netn(x).norm()=tensor(13.3393, grad_fn=<LinalgVectorNormBackward0>)\n",
      "8 netn(x).norm()=tensor(13.2729, grad_fn=<LinalgVectorNormBackward0>)\n",
      "9 netn(x).norm()=tensor(13.0163, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MyModelBN(torch.nn.Module):\n",
    "    def __init__(self, layer_size=[512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128*128*3  # number of channel\n",
    "        for s in layer_size:\n",
    "            layers.append(torch.nn.Linear(c, s, bias=False))\n",
    "            layers.append(torch.nn.BatchNorm1d(s)) #batch normalization\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102, bias=False))\n",
    "\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "x = torch.randn(10, 3, 128, 128)\n",
    "\n",
    "for n in range(10):\n",
    "    netn = MyModelBN([512] * n)\n",
    "    print(f\"{n} {netn(x).norm()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 netn(x).norm()=tensor(18.0582, grad_fn=<LinalgVectorNormBackward0>)\n",
      "1 netn(x).norm()=tensor(12.7572, grad_fn=<LinalgVectorNormBackward0>)\n",
      "2 netn(x).norm()=tensor(12.5842, grad_fn=<LinalgVectorNormBackward0>)\n",
      "3 netn(x).norm()=tensor(13.0634, grad_fn=<LinalgVectorNormBackward0>)\n",
      "4 netn(x).norm()=tensor(12.5875, grad_fn=<LinalgVectorNormBackward0>)\n",
      "5 netn(x).norm()=tensor(12.1720, grad_fn=<LinalgVectorNormBackward0>)\n",
      "6 netn(x).norm()=tensor(14.2014, grad_fn=<LinalgVectorNormBackward0>)\n",
      "7 netn(x).norm()=tensor(12.3908, grad_fn=<LinalgVectorNormBackward0>)\n",
      "8 netn(x).norm()=tensor(14.5423, grad_fn=<LinalgVectorNormBackward0>)\n",
      "9 netn(x).norm()=tensor(12.3325, grad_fn=<LinalgVectorNormBackward0>)\n",
      "10 netn(x).norm()=tensor(13.5382, grad_fn=<LinalgVectorNormBackward0>)\n",
      "11 netn(x).norm()=tensor(13.0165, grad_fn=<LinalgVectorNormBackward0>)\n",
      "12 netn(x).norm()=tensor(13.2984, grad_fn=<LinalgVectorNormBackward0>)\n",
      "13 netn(x).norm()=tensor(13.6450, grad_fn=<LinalgVectorNormBackward0>)\n",
      "14 netn(x).norm()=tensor(12.3181, grad_fn=<LinalgVectorNormBackward0>)\n",
      "15 netn(x).norm()=tensor(13.2800, grad_fn=<LinalgVectorNormBackward0>)\n",
      "16 netn(x).norm()=tensor(12.2531, grad_fn=<LinalgVectorNormBackward0>)\n",
      "17 netn(x).norm()=tensor(11.7266, grad_fn=<LinalgVectorNormBackward0>)\n",
      "18 netn(x).norm()=tensor(12.2116, grad_fn=<LinalgVectorNormBackward0>)\n",
      "19 netn(x).norm()=tensor(12.3550, grad_fn=<LinalgVectorNormBackward0>)\n",
      "20 netn(x).norm()=tensor(13.2683, grad_fn=<LinalgVectorNormBackward0>)\n",
      "21 netn(x).norm()=tensor(13.2982, grad_fn=<LinalgVectorNormBackward0>)\n",
      "22 netn(x).norm()=tensor(11.5966, grad_fn=<LinalgVectorNormBackward0>)\n",
      "23 netn(x).norm()=tensor(13.7284, grad_fn=<LinalgVectorNormBackward0>)\n",
      "24 netn(x).norm()=tensor(13.0782, grad_fn=<LinalgVectorNormBackward0>)\n",
      "25 netn(x).norm()=tensor(11.0853, grad_fn=<LinalgVectorNormBackward0>)\n",
      "26 netn(x).norm()=tensor(12.7664, grad_fn=<LinalgVectorNormBackward0>)\n",
      "27 netn(x).norm()=tensor(11.3610, grad_fn=<LinalgVectorNormBackward0>)\n",
      "28 netn(x).norm()=tensor(14.9791, grad_fn=<LinalgVectorNormBackward0>)\n",
      "29 netn(x).norm()=tensor(12.8191, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MyModelLN(torch.nn.Module):\n",
    "    def __init__(self, layer_size=[512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128*128*3  # number of channel\n",
    "        for s in layer_size:\n",
    "            layers.append(torch.nn.LayerNorm(s, bias=False))  # can put the normalization here\n",
    "\n",
    "            layers.append(torch.nn.Linear(c, s))\n",
    "            # layers.append(torch.nn.LayerNorm(s))  # can put the normalization here\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102, bias=False))\n",
    "\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "for n in range(30):\n",
    "    netn = MyModelLN([512] * n)\n",
    "    print(f\"{n} {netn(x).norm()=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
