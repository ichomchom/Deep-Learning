{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.4396e-01, -2.1168e+00, -1.3959e-01,  ...,  4.6358e-01,\n",
      "         -9.7295e-02,  2.2572e+00],\n",
      "        [ 1.8924e+00, -8.4603e-01,  5.5874e-01,  ...,  2.8706e-01,\n",
      "         -1.0950e+00,  2.2275e+00],\n",
      "        [ 5.7053e-01, -1.5183e+00, -2.0056e-01,  ...,  5.9660e-01,\n",
      "         -6.0130e-01,  1.7595e+00],\n",
      "        ...,\n",
      "        [ 5.6692e-01, -6.7182e-01,  1.3412e+00,  ...,  8.4679e-01,\n",
      "          1.7534e-01,  1.9191e+00],\n",
      "        [ 1.6121e-01, -9.9722e-01,  4.8937e-01,  ...,  5.4266e-01,\n",
      "         -2.1506e-01,  1.3463e+00],\n",
      "        [ 1.8894e-03, -1.9166e+00,  2.2276e-01,  ..., -6.3081e-02,\n",
      "         -8.6957e-01, -8.4825e-01]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MyModelLN(torch.nn.Module):\n",
    "    class Block(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels) -> None:\n",
    "            super().__init__()\n",
    "            self.linear = torch.nn.Linear(in_channels, out_channels) ## convolution\n",
    "            self.norm = torch.nn.LayerNorm(out_channels) ## normalization\n",
    "            self.relu = torch.nn.ReLU() ## ReLU\n",
    "            if in_channels != out_channels:\n",
    "                self.skip = torch.nn.Linear(in_channels, out_channels)\n",
    "            else:\n",
    "                self.skip = torch.nn.Identity()\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.relu(self.norm(self.linear(x)))\n",
    "            return self.skip(x) + y ## add skip connection\n",
    "            ## add x at the end changes this model from layer normalize network to residual network\n",
    "\n",
    "    def __init__(self, layer_size=[512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128*128*3  # number of channel\n",
    "        layers.append(torch.nn.Linear(c, 512, bias=False))\n",
    "        c = 512\n",
    "\n",
    "        for s in layer_size:\n",
    "            layers.append(self.Block(c, s))\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102, bias=False))\n",
    "\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "## Residual networks have skip connections in them that skip a bunch of sequential layers\n",
    "## The easiest way to implement is group linear layer norm and ReLU together in a layer\n",
    "# Then add skip connections around\n",
    "x = torch.randn(10, 3, 128, 128)\n",
    "net = MyModelLN([512] * 4)\n",
    "print(net(x))\n",
    "# for n in range(30):\n",
    "#     netn = MyModelLN([512] * n)\n",
    "#     print(f\"{n} {netn(x).norm()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2662,  0.2716, -0.0291,  ...,  0.3620,  1.0407, -1.2572],\n",
      "        [ 0.6800,  0.7348, -0.0885,  ...,  0.5710,  1.4504, -1.5437],\n",
      "        [-0.0852, -0.6613, -0.1593,  ...,  1.0970,  1.3200, -0.8434],\n",
      "        ...,\n",
      "        [-0.6748,  0.5078, -0.2419,  ...,  0.5855,  1.5774, -2.5527],\n",
      "        [-0.9504, -0.4432, -0.0864,  ...,  0.7035,  2.7226, -1.7251],\n",
      "        [-0.2463,  1.6944, -0.3771,  ...,  1.8689,  1.4131, -1.5296]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MyModelLN(torch.nn.Module):\n",
    "    class Block(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels) -> None:\n",
    "            super().__init__()\n",
    "            self.linear1 = torch.nn.Linear(\n",
    "                in_channels, out_channels)  # convolution\n",
    "            self.norm1 = torch.nn.LayerNorm(out_channels)  # normalization\n",
    "            self.relu1 = torch.nn.ReLU()  # ReLU\n",
    "            self.linear2 = torch.nn.Linear(\n",
    "                in_channels, out_channels)  # convolution\n",
    "            self.norm2 = torch.nn.LayerNorm(out_channels)  # normalization\n",
    "            self.relu2 = torch.nn.ReLU()  # ReLU\n",
    "            if in_channels != out_channels:\n",
    "                self.skip = torch.nn.Linear(in_channels, out_channels)\n",
    "            else:\n",
    "                self.skip = torch.nn.Identity()\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.relu1(self.norm1(self.linear1(x)))\n",
    "            y = self.relu2(self.norm2(self.linear2(x)))\n",
    "            return self.skip(x) + y  # add skip connection\n",
    "            # add x at the end changes this model from layer normalize network to residual network\n",
    "\n",
    "    def __init__(self, layer_size=[512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128*128*3  # number of channel\n",
    "        layers.append(torch.nn.Linear(c, 512, bias=False))\n",
    "        c = 512\n",
    "\n",
    "        for s in layer_size:\n",
    "            layers.append(self.Block(c, s))\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102, bias=False))\n",
    "\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Residual networks have skip connections in them that skip a bunch of sequential layers\n",
    "# The easiest way to implement is group linear layer norm and ReLU together in a layer\n",
    "# Then add skip connections around\n",
    "x = torch.randn(10, 3, 128, 128)\n",
    "net = MyModelLN([512] * 4)\n",
    "print(net(x))\n",
    "# for n in range(30):\n",
    "#     netn = MyModelLN([512] * n)\n",
    "#     print(f\"{n} {netn(x).norm()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9281,  2.2268, -2.1292,  ..., -1.2155,  0.2568, -0.9026],\n",
      "        [ 0.1421,  1.0655, -0.1878,  ..., -0.4254,  1.5252, -0.4275],\n",
      "        [ 1.9475,  0.8752, -1.3244,  ..., -0.3586,  0.1319,  0.4355],\n",
      "        ...,\n",
      "        [ 1.7794,  1.9763, -0.4189,  ..., -0.8238,  2.2739,  1.3529],\n",
      "        [ 1.5872,  1.0778, -0.0058,  ..., -1.1077,  1.9259, -0.0515],\n",
      "        [ 1.3999,  2.4352, -1.1638,  ..., -0.3932,  2.1526,  0.4430]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MyModelLN(torch.nn.Module):\n",
    "    class Block(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels) -> None:\n",
    "            super().__init__()\n",
    "            self.model = torch.nn.Sequential(torch.nn.Linear(in_channels, out_channels), \n",
    "                                             torch.nn.LayerNorm(out_channels), torch.nn.ReLU(), \n",
    "                                             torch.nn.Linear(in_channels, out_channels), \n",
    "                                             torch.nn.LayerNorm(out_channels), torch.nn.ReLU())\n",
    "            if in_channels != out_channels:\n",
    "                self.skip = torch.nn.Linear(in_channels, out_channels)\n",
    "            else:\n",
    "                self.skip = torch.nn.Identity()\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.skip(x) + self.model(x)\n",
    "\n",
    "    def __init__(self, layer_size=[512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128*128*3  # number of channel\n",
    "        layers.append(torch.nn.Linear(c, 512, bias=False))\n",
    "        c = 512\n",
    "\n",
    "        for s in layer_size:\n",
    "            layers.append(self.Block(c, s))\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102, bias=False))\n",
    "\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Residual networks have skip connections in them that skip a bunch of sequential layers\n",
    "# The easiest way to implement is group linear layer norm and ReLU together in a layer\n",
    "# Then add skip connections around\n",
    "x = torch.randn(10, 3, 128, 128)\n",
    "net = MyModelLN([512] * 4)\n",
    "print(net(x))\n",
    "# for n in range(30):\n",
    "#     netn = MyModelLN([512] * n)\n",
    "#     print(f\"{n} {netn(x).norm()=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
