{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Create a vector of zeros of size 5\n",
    "size = (128, 128)\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Resize(size), torchvision.transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.Flowers102(\n",
    "    \"../Module 1/flowers\", \"train\", transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.Flowers102(\n",
    "    \"../Module 1/flowers\", \"test\", transform=transform, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Flowers102\n",
       "    Number of datapoints: 1020\n",
       "    Root location: ../Module 1/flowers\n",
       "    split=train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0471, 0.0706, 0.0745,  ..., 0.1255, 0.4667, 0.5647],\n",
       "          [0.0667, 0.0667, 0.0549,  ..., 0.1333, 0.4824, 0.5647],\n",
       "          [0.0824, 0.0745, 0.0549,  ..., 0.1451, 0.5059, 0.5686],\n",
       "          ...,\n",
       "          [0.1059, 0.1059, 0.0863,  ..., 0.5020, 0.4902, 0.4706],\n",
       "          [0.1137, 0.1137, 0.1294,  ..., 0.5059, 0.4784, 0.4706],\n",
       "          [0.1020, 0.1176, 0.1176,  ..., 0.5020, 0.4745, 0.4667]],\n",
       " \n",
       "         [[0.0863, 0.1255, 0.1373,  ..., 0.1294, 0.3412, 0.3961],\n",
       "          [0.0941, 0.1098, 0.1059,  ..., 0.1294, 0.3490, 0.3922],\n",
       "          [0.0941, 0.0941, 0.0824,  ..., 0.1294, 0.3608, 0.3843],\n",
       "          ...,\n",
       "          [0.2000, 0.1804, 0.1333,  ..., 0.4235, 0.4118, 0.3922],\n",
       "          [0.2118, 0.2039, 0.2000,  ..., 0.4275, 0.4039, 0.3922],\n",
       "          [0.2078, 0.2196, 0.2196,  ..., 0.4196, 0.4078, 0.3765]],\n",
       " \n",
       "         [[0.0314, 0.0392, 0.0353,  ..., 0.0863, 0.4745, 0.5961],\n",
       "          [0.0392, 0.0353, 0.0235,  ..., 0.0980, 0.4902, 0.5922],\n",
       "          [0.0431, 0.0353, 0.0235,  ..., 0.1176, 0.5098, 0.5843],\n",
       "          ...,\n",
       "          [0.0471, 0.0588, 0.0588,  ..., 0.5686, 0.5569, 0.5373],\n",
       "          [0.0824, 0.0706, 0.1020,  ..., 0.5725, 0.5490, 0.5373],\n",
       "          [0.0745, 0.0941, 0.1020,  ..., 0.5608, 0.5490, 0.5294]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataLoader in module torch.utils.data.dataloader:\n",
      "\n",
      "class DataLoader(typing.Generic)\n",
      " |  DataLoader(dataset: torch.utils.data.dataset.Dataset[+_T_co], batch_size: Optional[int] = 1, shuffle: Optional[bool] = None, sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[List], Iterable[List], NoneType] = None, num_workers: int = 0, collate_fn: Optional[Callable[[List[~_T]], Any]] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Optional[Callable[[int], NoneType]] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int] = None, persistent_workers: bool = False, pin_memory_device: str = '', in_order: bool = True)\n",
      " |\n",
      " |  Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
      " |\n",
      " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      " |  iterable-style datasets with single- or multi-process loading, customizing\n",
      " |  loading order and optional automatic batching (collation) and memory pinning.\n",
      " |\n",
      " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
      " |\n",
      " |  Args:\n",
      " |      dataset (Dataset): dataset from which to load the data.\n",
      " |      batch_size (int, optional): how many samples per batch to load\n",
      " |          (default: ``1``).\n",
      " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      " |          at every epoch (default: ``False``).\n",
      " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
      " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
      " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
      " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
      " |          returns a batch of indices at a time. Mutually exclusive with\n",
      " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
      " |          and :attr:`drop_last`.\n",
      " |      num_workers (int, optional): how many subprocesses to use for data\n",
      " |          loading. ``0`` means that the data will be loaded in the main process.\n",
      " |          (default: ``0``)\n",
      " |      collate_fn (Callable, optional): merges a list of samples to form a\n",
      " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
      " |          map-style dataset.\n",
      " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      " |          into device/CUDA pinned memory before returning them.  If your data elements\n",
      " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      " |          see the example below.\n",
      " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
      " |          the size of dataset is not divisible by the batch size, then the last batch\n",
      " |          will be smaller. (default: ``False``)\n",
      " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      " |          from workers. Should always be non-negative. (default: ``0``)\n",
      " |      worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n",
      " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      " |          input, after seeding and before data loading. (default: ``None``)\n",
      " |      multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n",
      " |          ``None``, the default `multiprocessing context`_ of your operating system will\n",
      " |          be used. (default: ``None``)\n",
      " |      generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
      " |          by RandomSampler to generate random indexes and multiprocessing to generate\n",
      " |          ``base_seed`` for workers. (default: ``None``)\n",
      " |      prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n",
      " |          in advance by each worker. ``2`` means there will be a total of\n",
      " |          2 * num_workers batches prefetched across all workers. (default value depends\n",
      " |          on the set value for num_workers. If value of num_workers=0 default is ``None``.\n",
      " |          Otherwise, if value of ``num_workers > 0`` default is ``2``).\n",
      " |      persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n",
      " |          the worker processes after a dataset has been consumed once. This allows to\n",
      " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
      " |      pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\n",
      " |          ``True``.\n",
      " |      in_order (bool, optional): If ``False``, the data loader will not enforce that batches\n",
      " |          are returned in a first-in, first-out order. Only applies when ``num_workers > 0``. (default: ``True``)\n",
      " |\n",
      " |\n",
      " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
      " |               :ref:`multiprocessing-best-practices` on more details related\n",
      " |               to multiprocessing in PyTorch.\n",
      " |\n",
      " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
      " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
      " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
      " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
      " |               loading to avoid duplicate data.\n",
      " |\n",
      " |               However, if sharding results in multiple workers having incomplete last batches,\n",
      " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
      " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
      " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
      " |               cases in general.\n",
      " |\n",
      " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
      " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
      " |               `Multi-process data loading`_.\n",
      " |\n",
      " |  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
      " |               :ref:`data-loading-randomness` notes for random seed related questions.\n",
      " |\n",
      " |  .. warning:: Setting `in_order` to `False` can harm reproducibility and may lead to a skewed data\n",
      " |               distribution being fed to the trainer in cases with imbalanced data.\n",
      " |\n",
      " |  .. _multiprocessing context:\n",
      " |      https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      DataLoader\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+_T_co], batch_size: Optional[int] = 1, shuffle: Optional[bool] = None, sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[List], Iterable[List], NoneType] = None, num_workers: int = 0, collate_fn: Optional[Callable[[List[~_T]], Any]] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Optional[Callable[[int], NoneType]] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int] = None, persistent_workers: bool = False, pin_memory_device: str = '', in_order: bool = True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
      " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
      " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
      " |\n",
      " |  __len__(self) -> int\n",
      " |\n",
      " |  __setattr__(self, attr, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  check_worker_number_rationality(self)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  multiprocessing_context\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_iterator': typing.Optional[ForwardRef('_BaseDataL...\n",
      " |\n",
      " |  __orig_bases__ = (typing.Generic[+_T_co],)\n",
      " |\n",
      " |  __parameters__ = (+_T_co,)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...)\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.609244346618652\n",
      "4.691183090209961\n",
      "4.666364669799805\n",
      "4.642425537109375\n",
      "4.688011646270752\n",
      "4.725284576416016\n",
      "4.610206127166748\n",
      "4.642334461212158\n",
      "4.662638187408447\n",
      "4.690561771392822\n",
      "4.647457599639893\n",
      "4.659372806549072\n",
      "4.686764717102051\n",
      "4.669470310211182\n",
      "4.64767599105835\n",
      "4.638030052185059\n",
      "4.56156063079834\n",
      "4.6193037033081055\n",
      "4.582309246063232\n",
      "4.559229850769043\n",
      "4.601150035858154\n",
      "4.550866603851318\n",
      "4.579258441925049\n",
      "4.567873954772949\n",
      "4.6033830642700195\n",
      "4.556581020355225\n",
      "4.573458194732666\n",
      "4.561983108520508\n",
      "4.640350818634033\n",
      "4.4944939613342285\n",
      "4.595992088317871\n",
      "4.570327281951904\n",
      "4.4774041175842285\n",
      "4.521651268005371\n",
      "4.5165605545043945\n",
      "4.5041093826293945\n",
      "4.496799945831299\n",
      "4.506601810455322\n",
      "4.475484371185303\n",
      "4.4646897315979\n",
      "4.495852947235107\n",
      "4.490749359130859\n",
      "4.479709148406982\n",
      "4.492855548858643\n",
      "4.514074802398682\n",
      "4.480870246887207\n",
      "4.547335147857666\n",
      "4.46812105178833\n",
      "4.340383529663086\n",
      "4.389892101287842\n",
      "4.452930450439453\n",
      "4.393560409545898\n",
      "4.387499809265137\n",
      "4.441415786743164\n",
      "4.405380725860596\n",
      "4.3939595222473145\n",
      "4.421802043914795\n",
      "4.451624393463135\n",
      "4.433942794799805\n",
      "4.38875675201416\n",
      "4.428484916687012\n",
      "4.446247577667236\n",
      "4.44309139251709\n",
      "4.493499279022217\n",
      "4.308222770690918\n",
      "4.275707721710205\n",
      "4.370701789855957\n",
      "4.311635494232178\n",
      "4.348989486694336\n",
      "4.353719711303711\n",
      "4.302506923675537\n",
      "4.356237411499023\n",
      "4.3307671546936035\n",
      "4.364652633666992\n",
      "4.365840911865234\n",
      "4.245036602020264\n",
      "4.392931938171387\n",
      "4.385256767272949\n",
      "4.347968578338623\n",
      "4.355560302734375\n",
      "4.263537883758545\n",
      "4.271753311157227\n",
      "4.239388465881348\n",
      "4.215588092803955\n",
      "4.273892402648926\n",
      "4.202741622924805\n",
      "4.267900466918945\n",
      "4.333526134490967\n",
      "4.282424449920654\n",
      "4.216901779174805\n",
      "4.24459981918335\n",
      "4.282291412353516\n",
      "4.273802280426025\n",
      "4.324199676513672\n",
      "4.216959476470947\n",
      "4.274806499481201\n",
      "4.140605926513672\n",
      "4.215240955352783\n",
      "4.203310966491699\n",
      "4.193949222564697\n",
      "4.181747913360596\n",
      "4.224499702453613\n",
      "4.159773826599121\n",
      "4.202759265899658\n",
      "4.207225322723389\n",
      "4.206521511077881\n",
      "4.158443927764893\n",
      "4.162127494812012\n",
      "4.169422149658203\n",
      "4.239367485046387\n",
      "4.1757354736328125\n",
      "4.237496852874756\n",
      "4.144075393676758\n",
      "4.127224922180176\n",
      "4.161558628082275\n",
      "4.1449737548828125\n",
      "4.122397422790527\n",
      "3.997647762298584\n",
      "4.151431083679199\n",
      "4.110524654388428\n",
      "4.060589790344238\n",
      "4.143466949462891\n",
      "4.184962272644043\n",
      "4.174943923950195\n",
      "4.061379432678223\n",
      "4.135873317718506\n",
      "4.096583366394043\n",
      "4.128966331481934\n",
      "4.040438652038574\n",
      "4.069137096405029\n",
      "4.062641143798828\n",
      "4.075782299041748\n",
      "3.9876370429992676\n",
      "3.9853322505950928\n",
      "4.067227840423584\n",
      "4.0609354972839355\n",
      "4.028680801391602\n",
      "3.997175455093384\n",
      "4.027274131774902\n",
      "4.116600036621094\n",
      "4.028768062591553\n",
      "4.1600341796875\n",
      "4.091012477874756\n",
      "4.037302494049072\n",
      "3.941812038421631\n",
      "3.964484214782715\n",
      "3.9432570934295654\n",
      "3.987866163253784\n",
      "3.980041027069092\n",
      "4.023468971252441\n",
      "3.905750274658203\n",
      "3.9517226219177246\n",
      "4.080315113067627\n",
      "3.9648916721343994\n",
      "3.901421070098877\n",
      "4.078684329986572\n",
      "4.024544715881348\n",
      "3.985119104385376\n",
      "4.069809913635254\n",
      "4.0972514152526855\n"
     ]
    }
   ],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, layer_size = [512, 512, 512]) -> None:\n",
    "        super(MyModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128*128*3 # number of channel\n",
    "        for s in layer_size:\n",
    "            layers.append(torch.nn.Linear(c, s)) # each alayer go from c to s (from previous dimension to new dimension)\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102))\n",
    "        # initially we have a linear layer goes from 128*128*3 to 512\n",
    "        # then c = 512 then we go from 512 to 512\n",
    "        # then we go from 512 to 512 again\n",
    "        # then we goes from c to 102 which is our output\n",
    "        # the ReLU add between the last linear and the linear in the for loop\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "        # the sequential will create a network where the layer is called first\n",
    "        # then the next layer is called the next layer and then everything in the for loop is called\n",
    "        # in the end linear layer is called to 102 outputs\n",
    "        # sequential does is create a network where the output from one layer is fed into it as an input to the next layer until we don't have anything anymore\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = MyModel(layer_size=[])\n",
    "model.cuda()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "for epoch in range(10): \n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        pred = model(imgs)\n",
    "\n",
    "        loss_val = loss_fn(pred, labels)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print(loss_val.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
